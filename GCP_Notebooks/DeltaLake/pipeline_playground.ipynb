{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "_pipeline_module_file = '/home/mlops/airflow/dags/test_pipeline.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/mlops/airflow/dags/test_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_pipeline_module_file}\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import pyspark\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from spotipy import SpotifyClientCredentials\n",
    "import spotipy\n",
    "from airflow import DAG\n",
    "from airflow.decorators import task, task_group\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "spark = None\n",
    "db = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"testDAG\",\n",
    "    description='test_dag',\n",
    "    schedule_interval=None,\n",
    "    start_date= datetime.datetime(year=2022, month=2, day=1)\n",
    ") as dag:\n",
    "\n",
    "    if spark is None:\n",
    "    #Initialize Spark\n",
    "        builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "        spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "    @task(task_id='SILBER_ticketmaster_augmented_table_start')\n",
    "    def ingest_spotify_data_start():\n",
    "        #read deltalake table\n",
    "        ticketmaster = spark.read.format('delta').load('file:///home/mlops/project/DeltaLake/bronze_data/ticketmaster_table')\n",
    "\n",
    "        #read spotify data\n",
    "        spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id='7b4d7d2626b04188a79a02ecdda845e2', client_secret='e01be9dfdd854f74bfe0d0af10c5a4ee'))\n",
    "\n",
    "        print(f\"SPOTIPY: {spotify}\")\n",
    "        return ticketmaster.to_pandas_on_spark().to_dict()\n",
    "\n",
    "    @task(task_id='test')\n",
    "    def test(some):\n",
    "        print(\"SDFZUIOPOIUGF:\", some)\n",
    "        print(pd.DataFrame(some).head())\n",
    "        return some\n",
    "    @task_group(group_id='group_1')\n",
    "    def group_1(ticketmaster_dict):\n",
    "\n",
    "        # The @tasks below can be defined outside function `group_1`\n",
    "        # What matters is where they are referenced\n",
    "        # for index, tick in ticketmaster_df.iterrows():\n",
    "        tasks = list()\n",
    "        spotify_list = list()\n",
    "        print('TYPE:', type(ticketmaster_dict))\n",
    "        # for i in range(len(ticketmaster_dict['name'])):\n",
    "        for i in range(5):\n",
    "            @task(task_id=f\"task_1_{i}\")\n",
    "            def task_1(spotify_list, ticketmaster_dict, index):\n",
    "                # l.append(tick)\n",
    "                # l += tick['Age'][0]\n",
    "                print(ticketmaster_dict['name'][index])\n",
    "                return {\n",
    "                    'spotify_list': spotify_list,\n",
    "                    'ticketmaster_dict': ticketmaster_dict\n",
    "                }\n",
    "            # tasks are referenced here\n",
    "            # task_1_result = task_1(l, tick)\n",
    "            tasks.append(task_1(spotify_list, ticketmaster_dict, i))\n",
    "\n",
    "\n",
    "        # @task\n",
    "        # def task_1(l_tick):\n",
    "        #         l, ticketmaster_df = l_tick\n",
    "        #\n",
    "        #         print(l, ticketmaster_df)\n",
    "        #         return l\n",
    "        # task_1_result = task_1(l_tick)\n",
    "        # sending this list to `group_2`\n",
    "        return tasks\n",
    "\n",
    "\n",
    "    # group_1(ingest_spotify_data_start())\n",
    "    # test(ingest_spotify_data_start())\\\n",
    "\n",
    "    start = ingest_spotify_data_start()\n",
    "    do = group_1(start)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "import pyspark\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ticketmaster = spark.read.format('delta').load('file:///home/mlops/project/DeltaLake/bronze_data/ticketmaster_table')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "22/02/05 13:32:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/05 13:32:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/05 13:32:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "id\n",
      "locale\n",
      "distance\n",
      "units\n",
      "dates\n",
      "classifications\n",
      "_links\n",
      "price_min\n",
      "price_max\n",
      "_embedded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bitches = ticketmaster.to_pandas_on_spark().to_dict()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "for x in range(len(bitches)):\n",
    "    print(x)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "'BLVTH'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitches['name'][807]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}